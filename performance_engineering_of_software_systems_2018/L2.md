## Bentley Rules for Optimizing Work

Reducing the work of a program, does not automatically reduce its running time,
but it can redure.

#### Writing efficient programs - Jon Lous Bentley

### New Bentley Rules
#### Data structures
* Packing and encoding
* Augmentation
* Precomputation
* Compile-time initialization
* Caching
* Lazy evaluation
**** Sparcity

#### Loops
* Hoisting
* Sentinels
* Loop unrolling
* Loop fusion
* Eliminating wasted iterations

#### Logic
* Constant folding and propagation
* Common-subexpression elimination
* Algebraic identities
* Short-circuiting
* Ordering tests
* Creating a fast path
* Combining tests

#### Functions
* Inlining
* Tail-recursion elimination
* Coarsening recursion


### Data structures
The idea of `packing` is to store more than once data value in a machine word.
The related idea of encoding is to convert data values into a representation
requiring fewer bits.

Example: bitfields in struct

Sometimes unpacking and decoding are the optimization, depending on whether more work 
is involved moving the data or operating on it.


The idea of `augmentation` is to add information to a data structure to make common
operations do less work.

Example: adding a tail pointer to singly linked list (to make appending lists faster)


The idea of `precomputation` is to perform calculations in advance so as to avoid doing
them at "mission-critical" times.

Example: table lookup on a precomputed table. Or compute it once when the program starts, then
provide function to lookup into the table.


The idea of `compile-time initialization` is to store the values of constants during compilation,
saving work at execution time.

Idea: create large static tables by `metaprogramming` (write program that writes your table).
You can write your metaprograms in python etc, and use them in C/C++ if its easier.


The idea of `caching` is to store results that have been accesses recently so that the program
need not compute them again.

Example: store recently calculated values into cache.


The idea of exploiting `sparcity` is to avoid storing and computing on zeroes. "The fastest
way to compute is not to compute at all".

Example: compressed sparse row (CSR) for Matrix Multiplication. Or storing a static sparse graph.


### Logic
The idea of `constant folding and propagation` is to evaluate constant expressions and substitute
the result into further expressions, all during compilation.

Example: sufficiently high optimization level, most expressions are evaluated at compile-time.


The idea of `common-subexpression elimination` is to avoid computing the same expression 
multiple times by evaluating the expression once and storing the result for later use.


The idea of `exploiting algebraic identities` is to replace expensive algebraic expressions with
algebraic equivalents that require less work.

Example: remove sqrt() which is expensive with equivalent algebraic expression.


When performing a series of tests, the idea of `short-circuiting` is to stop evaluating as soon 
as you know the answer.

Example: early exit from function, but be aware about the common case, because it might be the
opposite.


Consider code that executes a sequence of logical tests. The idea of `ordering tests` is to
perform those that are move often "successful" - a particular alternative is selected by the
test - before tests that are rarely successful. Similarly, inexpensive tests should precede
expensive ones.


Creating a `fast path`. Idea is to exit the function quickly by knowing the result.


The idea of `combining tests` is to replace a sequence of tests with one test or switch.

Example: full adder - replace if/else with one test with shifted result to only do one switch.


### Loops
The goal of `hoisting` - also called `loop-invariant code motion` - is to avoid recomputing loop-
invariant code each time through the body of a loop.

Example: move out the factor (or something that can be only computed once) out of the loop.


`Sentinels` are special dummy values placed in a data structure to simplify the logic of
boundary conditions, and in particular, the handling of loop-exit tests.

Example: replace two checks to one check for overflow().


`Loop unrolling` - full loop unrolling and partial loop unrolling. Eliminate the checks for control
by doing more work (or all work) in loop body. By doing more work in body loop, compiler has more
opportunities to optimize (unless loop body is very large).
* Lower number of instructions in loop control code
* Enables more compiler optimizations
* Unrolling too much can cause poor use of instruction cache


The idea of `loop fusion` - also called `jamming` - is to combine multiple loops over the same index
range into a single loop body, thereby saving the overhead of loop control.


The idea of `eliminating wasted iterations` is to modify loop bounds to avoid executing loop
iterations over essentially empty loop bodies.

Example: when a lot of loop iterations are empty, remove them. (1:15 for example)


### Functions
The idea of `inlining` is to avoid the overhead of a function call by replacing a call to
the function with the body of the function itself.

Example: in C, declare function as `static inline`. Or do it manually (compilers do that too).


The idea of `tail-recursion elimination` is to replace a recursive call that occurs as the
last step of a function with a branch, saving function-call overhead.


The idea of `coarsening recursion` is to increase the size of the base case and handle it
with more efficient code that avoids function-call overhead.


### Closing advice
* Avoid premature optimization. First get correct working code. Then optimize, preserving
correctness by regression testing.

* Reducing the work of a program does not necessarily decrease its running time, but it
is a good heuristic.

* The compiler automates many low-level optimizations.

* To tell if the compiler is actually performing a particular optimization, look at the
assembly code.




